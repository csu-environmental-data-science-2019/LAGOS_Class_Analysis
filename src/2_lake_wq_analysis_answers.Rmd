---
title: "Lake Water Quality Analysis"
author: "Matthew Ross"
date: "9/17/2019"
output: html_document
---



```{r setup, include=FALSE}
library(tidyverse) # Tidy packages
library(sf) #Spatial package that can read and create shapefiles 
library(mapview) #Interactive maps
library(LAGOSNE) #Lots and lots of clean lake data
library(USAboundaries) #USA states and counties
library(lubridate) #For dealing with date and time
library(ggthemes) #fancy themes

#globally apply theme few
theme_set(theme_few())
```


# LAGOS Analysis


## Loading in data


### First download and then specifically grab the locus (or site lat longs)
```{r data-read}
#Lagos download script
#lagosne_get(dest_folder = LAGOSNE:::lagos_path(),overwrite=T)

#Load in lagos
lagos <- lagosne_load()


#Grab the lake centroid info
lake_centers <- lagos$locus

# Make an sf object 
spatial_lakes <- st_as_sf(lake_centers,coords=c('nhd_long','nhd_lat'),
                          crs=4326)

#Grab the water quality data
nutr <- lagos$epi_nutr

#Look at column names
#names(nutr)
```

### Subset columns nutr to only keep key info that we want


```{r}
clarity_only <- nutr %>%
  select(lagoslakeid,sampledate,chla,doc,secchi) %>%
  mutate(sampledate = as.character(sampledate) %>% ymd(.))

```


### Keep sites with at least 200 observations 

```{r}

#Look at the number of rows of dataset
#nrow(clarity_only)

chla_secchi <- clarity_only %>%
  filter(!is.na(chla),
         !is.na(secchi))

# How many observatiosn did we lose?
# nrow(clarity_only) - nrow(chla_secchi)


# Keep only the lakes with at least 200 observations of secchi and chla
chla_secchi_200 <- chla_secchi %>%
  group_by(lagoslakeid) %>%
  mutate(count = n()) %>%
  filter(count > 200)


```


### Join water quality data to spatial data

```{r}
spatial_200 <- inner_join(spatial_lakes,chla_secchi_200 %>%
                            distinct(lagoslakeid,.keep_all=T),
                          by='lagoslakeid')


```

### Mean Chl_a map

```{r}
### Take the mean chl_a and secchi by lake

mean_values_200 <- chla_secchi_200 %>%
  # Take summary by lake id
  group_by(lagoslakeid) %>%
  # take mean chl_a per lake id
  summarize(mean_chl = mean(chla,na.rm=T),
            mean_secchi=mean(secchi,na.rm=T)) %>%
  #Get rid of NAs
  filter(!is.na(mean_chl),
         !is.na(mean_secchi)) %>%
  # Take the log base 10 of the mean_chl
  mutate(log10_mean_chl = log10(mean_chl))

#Join datasets
mean_spatial <- inner_join(spatial_lakes,mean_values_200,
                          by='lagoslakeid') 

#Make a map
mapview(mean_spatial,zcol='log10_mean_chl')
```


# Class work

## 1) What is the correlation between Secchi Disk Depth and Chlorophyll a for
sites with at least 200 observations?

- Here, I just want a plot of chla vs secchi for all sites 

### My answer: 


```{r}
ggplot(chla_secchi_200,aes(x=chla,y=secchi)) + 
  geom_point() + 
  theme_few() + 
  scale_x_log10()

```

### Your answers: 

Here someone took the average correlation, which also works to illustrate the
point that increased algal biomass lowers lake clarity. 

```{r}
ggplot(mean_spatial, aes(x = mean_chl, y = mean_secchi))+
  geom_point()
```



## 2) What states have the most data? (I should have clarified I meant clarity
only data) 

### 2a) First you will need to make a lagos spatial dataset that has the total 
number of counts per site.


### My answer:
```{r}
clarity_counts <- clarity_only %>%
  group_by(lagoslakeid) %>%
  summarize(count=n()) 

spatial_counts <- inner_join(spatial_lakes,clarity_counts,by='lagoslakeid')
```

### Your answers:

```{r, eval=F}
count_nutr <- nutr %>%
  group_by(lagoslakeid) %>%
  summarise(count = n())

#Here the distinct call is unnecessary since summarize already keeps only
#unique lagoslakeids
spatial_nutr <- inner_join(spatial_lakes,count_nutr %>%
                            distinct(lagoslakeid,.keep_all=T),
                          by='lagoslakeid')
```

### 2b) Second, you will need to join this point dataset to the us_boundaries 
data. 


### My answer (and most of you as well)
```{r}

states <- us_states()
state_counts <- st_join(states, spatial_counts)

```


### 2c) Then you will want to group by state and sum all the observations in that
state and arrange that data from most to least total observations per state. 


### My answer (and most of you)

```{r}

state_summary <- state_counts %>%
  group_by(name) %>%
  summarize(state_obs = sum(count))


state_summary %>%
  arrange(-state_obs)
```


##3 Is there a spatial pattern in Secchi disk depth for lakes with at least 200 
observations?


### My answer


```{r}
## Your code here
mapview(spatial_200,zcol = 'secchi')

```




